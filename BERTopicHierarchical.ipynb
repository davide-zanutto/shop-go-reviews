{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from openai import AzureOpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "project_id = 'ingka-online-analytics-prod'\n",
    "dataset_id = 'app_data_v2'\n",
    "table_id = 'app_surveys'\n",
    "\n",
    "table_ref = f'{project_id}.{dataset_id}.{table_id}'\n",
    "\n",
    "## Query to test with a fixed number of reviews per day\n",
    "\n",
    "num_reviews = 10000\n",
    "num_reviews_per_day = 300\n",
    "\n",
    "query_test = f\"\"\"\n",
    "    WITH ranked_reviews AS (\n",
    "        SELECT \n",
    "            date, \n",
    "            answer_translated,\n",
    "            ROW_NUMBER() OVER (PARTITION BY date ORDER BY date DESC) as row_num\n",
    "        FROM {table_ref}\n",
    "        WHERE answer_translated IS NOT NULL AND rating != 0\n",
    "    )\n",
    "    SELECT *\n",
    "    FROM ranked_reviews\n",
    "    WHERE row_num <= {num_reviews_per_day}\n",
    "    ORDER BY date DESC\n",
    "    LIMIT {num_reviews}\n",
    "\"\"\"\n",
    "\n",
    "## With 6 months of data, the number of reviews will be between 2M and 3M\n",
    "### Of this, only around 200k have a non-null answer_translated\n",
    "\n",
    "query_1_month = f\"\"\"\n",
    "    SELECT\n",
    "        date, \n",
    "        answer_translated\n",
    "    FROM {table_ref}\n",
    "    WHERE date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH) AND current_date()\n",
    "        AND answer_translated IS NOT NULL AND rating != 0\n",
    "    ORDER BY date DESC\n",
    "\"\"\"\n",
    "\n",
    "query_3_months = f\"\"\"\n",
    "    SELECT\n",
    "        date, \n",
    "        answer_translated\n",
    "    FROM {table_ref}\n",
    "    WHERE date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 3 MONTH) AND current_date()\n",
    "        AND answer_translated IS NOT NULL AND rating != 0\n",
    "    ORDER BY date DESC\n",
    "\"\"\"\n",
    "\n",
    "query_6_months = f\"\"\"\n",
    "    SELECT\n",
    "        date, \n",
    "        answer_translated\n",
    "    FROM {table_ref}\n",
    "    WHERE date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 6 MONTH) AND current_date()\n",
    "        AND answer_translated IS NOT NULL AND rating != 0\n",
    "    ORDER BY date DESC\n",
    "\"\"\"\n",
    "\n",
    "query_job = client.query(query_1_month)\n",
    "\n",
    "reviews = [row['answer_translated'] for row in query_job]\n",
    "timestamps = [row['date'] for row in query_job]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identify and remove non-english reviews\n",
    "### For 6 months of data, this takes around 10 minutes \n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "print(\"Reviews before processing: \", len(reviews))\n",
    "\n",
    "filtered_reviews = []\n",
    "filtered_timestamps = []\n",
    "removed_reviews = []\n",
    "\n",
    "for review, timestamp in zip(reviews, timestamps):\n",
    "    try:\n",
    "        if detect(review) == 'en' and len(review.split()) > 1 and len(review) >= 10:\n",
    "            filtered_reviews.append(review)\n",
    "            filtered_timestamps.append(timestamp)\n",
    "        else:\n",
    "            removed_reviews.append(review)\n",
    "    except:\n",
    "        removed_reviews.append(review)\n",
    "\n",
    "print(\"Removed reviews:\")\n",
    "for review in removed_reviews:\n",
    "    print(review)\n",
    "\n",
    "reviews = filtered_reviews\n",
    "timestamps = filtered_timestamps\n",
    "\n",
    "print(\"Reviews after processing: \", len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "formatted_timestamps = [ts.strftime(\"%Y-%m-%d\") for ts in timestamps]\n",
    "\n",
    "stop_words = set(stopwords.words('english')).union(set(ENGLISH_STOP_WORDS))\n",
    "\n",
    "processed_reviews = [' '.join([word for word in word_tokenize(review.lower()) if word.isalnum() and word not in stop_words]) for review in reviews]\n",
    "\n",
    "## Limiting the number of topics with nr_topics does not work\n",
    "nr_topics_before = 'Auto'\n",
    "topic_model = BERTopic()\n",
    "\n",
    "# Fit the model on the reviews\n",
    "topics, probabilities = topic_model.fit_transform(reviews)\n",
    "\n",
    "nr_topics_after = 'auto'\n",
    "\n",
    "# Further reduce topics if needed\n",
    "# topic_model.reduce_topics(reviews, nr_topics=nr_topics_after)\n",
    "\n",
    "topics_over_time = topic_model.topics_over_time(reviews, formatted_timestamps, datetime_format=\"%Y-%m-%d\", nr_bins=10)\n",
    "topics = topic_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = topic_model.get_topic_info()\n",
    "all_topic_names = '; '.join(topic_info['Name'])\n",
    "all_topic_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_topics = len(topics)\n",
    "number_of_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_topics, Z = topic_model.hierarchical_topics(processed_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = topic_model.get_topic_tree(hierarchical_topics)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_at_depth(df, depth):\n",
    "    from collections import deque\n",
    "    \n",
    "    # Build adjacency list with stored distance (no accumulation)\n",
    "    adjacency = {}\n",
    "    for _, row in df.iterrows():\n",
    "        adjacency[row['Parent_ID']] = [\n",
    "            (row['Child_Left_ID'], row['Child_Left_Name'], row['Distance']),\n",
    "            (row['Child_Right_ID'], row['Child_Right_Name'], row['Distance'])\n",
    "        ]\n",
    "    \n",
    "    root_id = df.iloc[0]['Parent_ID']\n",
    "    root_name = df.iloc[0]['Parent_Name']\n",
    "    \n",
    "    # BFS\n",
    "    queue = deque([(root_id, root_name, 0)])  # (id, name, depth)\n",
    "    result = []\n",
    "    \n",
    "    while queue:\n",
    "        node_id, node_name, curr_depth = queue.popleft()\n",
    "        children = adjacency.get(node_id, [])\n",
    "        \n",
    "        for child_id, child_name, child_distance in children:\n",
    "            child_depth = curr_depth + 1\n",
    "            if child_depth == depth:\n",
    "                result.append((child_id, child_name, child_distance))\n",
    "            elif child_depth < depth:\n",
    "                queue.append((child_id, child_name, child_depth))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_at_depth = get_topics_at_depth(hierarchical_topics, 3)\n",
    "for topic in topics_at_depth:\n",
    "    print(f\"ID: {topic[0]}, Name: {topic[1]}, Distance: {topic[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_topics(df, topics, threshold):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    # Build child->parent and parent->children maps\n",
    "    child_to_parent = {}\n",
    "    parent_to_children = defaultdict(list)\n",
    "    for _, row in df.iterrows():\n",
    "        p_id, p_name, p_dist = row['Parent_ID'], row['Parent_Name'], row['Distance']\n",
    "        cl_id, cl_name = row['Child_Left_ID'], row['Child_Left_Name']\n",
    "        cr_id, cr_name = row['Child_Right_ID'], row['Child_Right_Name']\n",
    "        \n",
    "        child_to_parent[cl_id] = (p_id, p_name, p_dist)\n",
    "        child_to_parent[cr_id] = (p_id, p_name, p_dist)\n",
    "        parent_to_children[p_id].append((cl_id, cl_name, p_dist))\n",
    "        parent_to_children[p_id].append((cr_id, cr_name, p_dist))\n",
    "\n",
    "    # Start with the current topics in a set\n",
    "    final_topics = set(topics)\n",
    "    \n",
    "    # Below-threshold topics\n",
    "    below_threshold = [t for t in topics if t[2] < threshold]\n",
    "\n",
    "    # For each below-threshold topic, pair it with another topic of the same distance,\n",
    "    # remove both, then add the parent. Then remove the highest-distance topic and add its children.\n",
    "    for bt_id, bt_name, bt_dist in below_threshold:\n",
    "        if (bt_id, bt_name, bt_dist) not in final_topics:\n",
    "            continue\n",
    "\n",
    "        # Find another topic with the same distance\n",
    "        same_dist_candidates = [\n",
    "            t for t in final_topics\n",
    "            if t[2] == bt_dist and t != (bt_id, bt_name, bt_dist)\n",
    "        ]\n",
    "        if not same_dist_candidates:\n",
    "            continue\n",
    "\n",
    "        # Remove the below-threshold topic and its same-distance candidate\n",
    "        same_dist_topic = same_dist_candidates[0]\n",
    "        final_topics.remove((bt_id, bt_name, bt_dist))\n",
    "        final_topics.remove(same_dist_topic)\n",
    "\n",
    "        # Add the parent of the below-threshold topic\n",
    "        parent = child_to_parent.get(bt_id, (bt_id, bt_name, bt_dist))\n",
    "        final_topics.add(parent)\n",
    "\n",
    "        # Find the highest-distance topic, remove it, and add its children\n",
    "        if final_topics:\n",
    "            highest_topic = max(final_topics, key=lambda x: x[2])\n",
    "            final_topics.remove(highest_topic)\n",
    "            h_id, h_name, h_dist = highest_topic\n",
    "            for ch_id, ch_name, ch_dist in parent_to_children.get(h_id, []):\n",
    "                final_topics.add((ch_id, ch_name, ch_dist))\n",
    "\n",
    "    return list(final_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = adjust_topics(hierarchical_topics, topics_at_depth, 1)\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtopics_for_topics(df, topics, threshold):\n",
    "    \"\"\"\n",
    "    For each topic in 'topics', find subtopics by going up to 2 levels down a binary tree:\n",
    "      1) If the topic's direct children (level 1) have distance < threshold, return those 2 children.\n",
    "      2) Otherwise, go one more level (level 2) and return those 4 descendants\n",
    "\n",
    "    Returns a dict: { \"topic_id:topic_name\": [ (child_id, child_name, distance), ... ] }\n",
    "    \"\"\"\n",
    "    from collections import defaultdict, deque\n",
    "\n",
    "    parent_to_children = defaultdict(list)\n",
    "    for _, row in df.iterrows():\n",
    "        p_id = row['Parent_ID']\n",
    "        parent_to_children[p_id].append((row['Child_Left_ID'], row['Child_Left_Name'], row['Distance']))\n",
    "        parent_to_children[p_id].append((row['Child_Right_ID'], row['Child_Right_Name'], row['Distance']))\n",
    "\n",
    "    def collect_descendants(root_id, max_level=2):\n",
    "        queue = deque([(root_id, 0)])\n",
    "        levels_nodes = defaultdict(list)\n",
    "        while queue:\n",
    "            node_id, lvl = queue.popleft()\n",
    "            for (cid, cname, cdist) in parent_to_children.get(node_id, []):\n",
    "                levels_nodes[lvl + 1].append((cid, cname, cdist))\n",
    "                if lvl + 1 < max_level:\n",
    "                    queue.append((cid, lvl + 1))\n",
    "\n",
    "        for level in range(1, max_level + 1):\n",
    "            nodes = levels_nodes.get(level, [])\n",
    "            if not nodes:\n",
    "                return []\n",
    "            if any(n[2] < threshold for n in nodes) or level == max_level:\n",
    "                return nodes\n",
    "        return []\n",
    "\n",
    "    result = {}\n",
    "    for (t_id, t_name, t_dist) in topics:\n",
    "        subtopics = collect_descendants(t_id)\n",
    "        result[(t_id, t_name)] = subtopics\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtopics = get_subtopics_for_topics(hierarchical_topics, topics, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_leaves(topic_structure, hierarchical_topics):\n",
    "    \"\"\"\n",
    "    For each subtopic, get its ID and retrieve the 'Topics' attribute from the hierarchical_topics dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - topic_structure: Dictionary containing topics and their subtopics.\n",
    "    - hierarchical_topics: DataFrame containing hierarchical topic information.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with subtopic IDs as keys and their 'Topics' attributes as values.\n",
    "    \"\"\"\n",
    "    subtopic_topics = {}\n",
    "\n",
    "    for main_topic, subtopics in topic_structure.items():\n",
    "        for subtopic in subtopics:\n",
    "            subtopic_id = subtopic[0]\n",
    "            # Find the row in the dataframe with the matching subtopic ID\n",
    "            row = hierarchical_topics[hierarchical_topics['Parent_ID'] == subtopic_id]\n",
    "            if not row.empty:\n",
    "                subtopic_topics[subtopic_id] = row.iloc[0]['Topics']\n",
    "            else:\n",
    "                subtopic_topics[subtopic_id] = [int(subtopic_id)]\n",
    "\n",
    "    return subtopic_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtopic_topics = get_leaves(subtopics, hierarchical_topics)\n",
    "print(subtopic_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews_by_subtopic(subtopic_topics, topic_model, documents):\n",
    "    \"\"\"\n",
    "    Get reviews associated with each subtopic ID in the subtopic_topics dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - subtopic_topics: Dictionary with subtopic IDs as keys and list of topic IDs as values.\n",
    "    - topic_model: Trained BERTopic model.\n",
    "    - documents: List of all input documents to the BERTopic model.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with subtopic IDs as keys and list of reviews as values.\n",
    "    \"\"\"\n",
    "    subtopic_reviews = {}\n",
    "\n",
    "    # Get topic assignments for each document\n",
    "    topic_assignments = topic_model.transform(documents)[0]\n",
    "\n",
    "    for subtopic_id, topic_ids in subtopic_topics.items():\n",
    "        # Filter documents based on the topic IDs\n",
    "        associated_docs = [doc for doc, assigned_topic in zip(documents, topic_assignments) if assigned_topic in topic_ids]\n",
    "        subtopic_reviews[subtopic_id] = associated_docs\n",
    "\n",
    "    for subtopic_id, reviews in subtopic_reviews.items():\n",
    "        print(f\"Subtopic ID {subtopic_id} has {len(reviews)} reviews.\")\n",
    "\n",
    "    return subtopic_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtopic_reviews = get_reviews_by_subtopic(subtopic_topics, topic_model, reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    api_version=\"2023-07-01-preview\",\n",
    "    azure_endpoint=\"https://derai-vision.openai.azure.com/\",\n",
    ")\n",
    "\n",
    "model = \"gpt-4o-mini\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_keyword(cluster_words):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a helpful expert summarizer that identifies and generates a concise, broad topic word for each cluster of words.\\n\"\n",
    "                \"The topic word should capture the essence of all the words in the cluster.\\n\"\n",
    "                \"Merge similar or related words into a single, broader category.\\n\"\n",
    "                \"Use singular words unless a plural form is necessary.\\n\"                \n",
    "                \"Use only one word. 2 or 3 words can be used only when they are part of a composite word and are better to represent the idea of the topic (e.g.: ease of use).\\n\"\n",
    "                \"If you identify a verb as a topic, use the noun version (e.g., use 'order' instead of 'ordering').\\n\"\n",
    "                \"Generalize the topic word; for example, if you encounter 'saleswoman' or 'salesman', abstract it to 'staff'.\\n\"\n",
    "                \"Provide the output as a single word.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Please read the following cluster of words carefully and generate a single topic word that captures the essence of all the words.\\n\"\n",
    "                \"The topic word should be broad and general, capturing the essence of the cluster's main points without being overly specific or redundant.\\n\"\n",
    "                \"The topics could be either nouns that refers to a certain characteristic of the product of spefic features or parts of the product (e.g.: click & collect, email redeem, etc.)\\n\"\n",
    "                f\"Cluster: {', '.join(cluster_words)}\\n\"\n",
    "                \"Topic word(s):\"\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = ' '\n",
    "    \n",
    "    # Generate the topic word using the language model\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=5,\n",
    "        temperature=0.4,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "    )\n",
    "\n",
    "    # Extract and return the topic word\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtopic_keyword(topic_keyword, cluster_words):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a helpful expert summarizer that identifies and generates a concise, broad subtopic word for each cluster of words.\\n\"\n",
    "                \"The topic word should capture the essence of all the words in the cluster.\\n\"\n",
    "                \"The words you choose can be specific, since they are a specialization of a broader topic word.\\n\" \n",
    "                \"Use singular words unless a plural form is necessary.\\n\"                \n",
    "                \"Use only one word unless 2 or 3 words are better to represent the idea of the subtopic.\\n\"\n",
    "                \"If you identify a verb as a subtopic, use the noun version (e.g., use 'order' instead of 'ordering').\\n\"\n",
    "                \"Generalize the topic word; for example, if you encounter 'saleswoman' or 'salesman', abstract it to 'staff'.\\n\"\n",
    "                f\"Provide the output as: '{topic_keyword} - <Subtopic word>'.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Please read the following cluster of words carefully and generate a single subtopic word that captures the essence of all the words.\\n\"\n",
    "                \"The subtopic is a specification of the broader topic, therefore it should be about an aspect that the customers mention and that is related to the broader topic.\\n\"\n",
    "                \"The topics could be either nouns that refers to a certain characteristic of the product of spefic features or parts of the product (e.g.: click & collect, email redeem, etc.)\\n\"\n",
    "                f\"The broader topic word is: {topic_keyword}\\n\"\n",
    "                f\"Cluster: {', '.join(cluster_words)}\\n\"\n",
    "                \"Topic word(s):\"\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = ' '\n",
    "\n",
    "    # Generate the topic word using the language model\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=10,\n",
    "        temperature=0.4,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "    )\n",
    "\n",
    "    # Extract and return the topic word\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def create_json_structure(subtopics_structure, subtopic_reviews, output_file):\n",
    "    \"\"\"\n",
    "    Create a JSON structure with topics, subtopics, and reviews, and save it to a file.\n",
    "\n",
    "    Parameters:\n",
    "    - subtopics_structure: Dictionary containing topics and their subtopics.\n",
    "    - subtopic_reviews: Dictionary with subtopic IDs as keys and list of reviews as values.\n",
    "    - output_file: Path to the output JSON file.\n",
    "    \"\"\"\n",
    "    json_structure = {}\n",
    "\n",
    "    for main_topic, subtopics in subtopics_structure.items():\n",
    "        topic_name = main_topic[1]\n",
    "        topic_keyword = get_topic_keyword(topic_name)\n",
    "        json_structure[main_topic[1]] = {\n",
    "            \"keyword\": topic_keyword,\n",
    "            \"subtopics\": {}\n",
    "        }\n",
    "\n",
    "        print(f\"Processing main topic: {main_topic[0]} - {topic_name}\")\n",
    "\n",
    "        for subtopic in subtopics:\n",
    "            subtopic_id = subtopic[0]\n",
    "            subtopic_name = subtopic[1]\n",
    "            subtopic_keyword = get_subtopic_keyword(topic_keyword, subtopic_name)\n",
    "            reviews = subtopic_reviews.get(subtopic_id, [])\n",
    "\n",
    "            json_structure[main_topic[1]][\"subtopics\"][subtopic_name] = {\n",
    "                \"subtopic_keyword\": subtopic_keyword,\n",
    "                \"reviews\": reviews\n",
    "            }\n",
    "\n",
    "            print(f\"  Subtopic ID: {subtopic_id} - {subtopic_name}\")\n",
    "            print(f\"    Keyword: {subtopic_keyword}\")\n",
    "            print(f\"    Number of reviews: {len(reviews)}\")\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(json_structure, f, indent=4)\n",
    "\n",
    "    print(f\"JSON structure saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'output.json'\n",
    "create_json_structure(subtopics, subtopic_reviews, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run app.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-reviews-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
