{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from openai import AzureOpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "project_id = 'ingka-online-analytics-prod'\n",
    "dataset_id = 'app_data_v2'\n",
    "table_id = 'app_surveys'\n",
    "\n",
    "table_ref = f'{project_id}.{dataset_id}.{table_id}'\n",
    "\n",
    "## Query to test with a fixed number of reviews per day\n",
    "\n",
    "num_reviews = 1000\n",
    "num_reviews_per_day = 300\n",
    "\n",
    "query_test = f\"\"\"\n",
    "    WITH ranked_reviews AS (\n",
    "        SELECT \n",
    "            date, \n",
    "            answer_translated,\n",
    "            ROW_NUMBER() OVER (PARTITION BY date ORDER BY date DESC) as row_num\n",
    "        FROM {table_ref}\n",
    "        WHERE answer_translated IS NOT NULL AND rating != 0\n",
    "    )\n",
    "    SELECT *\n",
    "    FROM ranked_reviews\n",
    "    WHERE row_num <= {num_reviews_per_day}\n",
    "    ORDER BY date DESC\n",
    "    LIMIT {num_reviews}\n",
    "\"\"\"\n",
    "\n",
    "## With 6 months of data, the number of reviews will be between 2M and 3M\n",
    "### Of this, only around 200k have a non-null answer_translated\n",
    "\n",
    "query_1_month = f\"\"\"\n",
    "    SELECT\n",
    "        date, \n",
    "        answer_translated\n",
    "    FROM {table_ref}\n",
    "    WHERE date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH) AND current_date()\n",
    "        AND answer_translated IS NOT NULL AND rating != 0\n",
    "    ORDER BY date DESC\n",
    "\"\"\"\n",
    "\n",
    "query_3_months = f\"\"\"\n",
    "    SELECT\n",
    "        date, \n",
    "        answer_translated\n",
    "    FROM {table_ref}\n",
    "    WHERE date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 3 MONTH) AND current_date()\n",
    "        AND answer_translated IS NOT NULL AND rating != 0\n",
    "    ORDER BY date DESC\n",
    "\"\"\"\n",
    "\n",
    "query_6_months = f\"\"\"\n",
    "    SELECT\n",
    "        date, \n",
    "        answer_translated\n",
    "    FROM {table_ref}\n",
    "    WHERE date BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 6 MONTH) AND current_date()\n",
    "        AND answer_translated IS NOT NULL AND rating != 0\n",
    "    ORDER BY date DESC\n",
    "\"\"\"\n",
    "\n",
    "query_job = client.query(query_3_months)\n",
    "\n",
    "reviews = [row['answer_translated'] for row in query_job]\n",
    "timestamps = [row['date'] for row in query_job]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identify and remove non-english reviews\n",
    "### For 6 months of data, this takes around 10 minutes \n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "print(\"Reviews before processing: \", len(reviews))\n",
    "\n",
    "filtered_reviews = []\n",
    "filtered_timestamps = []\n",
    "removed_reviews = []\n",
    "\n",
    "for review, timestamp in zip(reviews, timestamps):\n",
    "    try:\n",
    "        if detect(review) == 'en' and len(review.split()) > 1 and len(review) >= 10:\n",
    "            filtered_reviews.append(review)\n",
    "            filtered_timestamps.append(timestamp)\n",
    "        else:\n",
    "            removed_reviews.append(review)\n",
    "    except:\n",
    "        removed_reviews.append(review)\n",
    "\n",
    "print(\"Removed reviews:\")\n",
    "for review in removed_reviews:\n",
    "    print(review)\n",
    "\n",
    "reviews = filtered_reviews\n",
    "timestamps = filtered_timestamps\n",
    "\n",
    "print(\"Reviews after processing: \", len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "formatted_timestamps = [ts.strftime(\"%Y-%m-%d\") for ts in timestamps]\n",
    "\n",
    "stop_words = set(stopwords.words('english')).union(set(ENGLISH_STOP_WORDS))\n",
    "\n",
    "processed_reviews = [' '.join([word for word in word_tokenize(review.lower()) if word.isalnum() and word not in stop_words]) for review in reviews]\n",
    "\n",
    "## Limiting the number of topics with nr_topics does not work\n",
    "nr_topics_before = 'Auto'\n",
    "topic_model = BERTopic()\n",
    "\n",
    "# Fit the model on the reviews\n",
    "topics, probabilities = topic_model.fit_transform(reviews)\n",
    "\n",
    "nr_topics_after = 'auto'\n",
    "\n",
    "# Further reduce topics if needed\n",
    "# topic_model.reduce_topics(reviews, nr_topics=nr_topics_after)\n",
    "\n",
    "topics_over_time = topic_model.topics_over_time(reviews, formatted_timestamps, datetime_format=\"%Y-%m-%d\", nr_bins=10)\n",
    "topics = topic_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = topic_model.get_topic_info()\n",
    "all_topic_names = '; '.join(topic_info['Name'])\n",
    "all_topic_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_topics = len(topics)\n",
    "number_of_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap(top_n_topics = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def get_topic_embedding(topic_id):\n",
    "    words = [word for word, _ in topic_model.get_topic(topic_id)]\n",
    "    embeddings = sentence_model.encode(words)\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "def topic_cosine_similarity(t1, t2):\n",
    "    v1 = get_topic_embedding(t1)\n",
    "    v2 = get_topic_embedding(t2)\n",
    "    return cosine_similarity([v1], [v2])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = topic_model.get_topic_freq()\n",
    "valid_topics = topics[topics.Topic != -1].sort_values(by=\"Count\", ascending=False)\n",
    "\n",
    "threshold = 0.75\n",
    "\n",
    "# Start by selecting the first 8 topics\n",
    "top_topics = valid_topics.head(8).Topic.tolist()\n",
    "index = 8  # Next topic candidate\n",
    "\n",
    "while True:\n",
    "    changed = False\n",
    "    for i in range(len(top_topics)):\n",
    "        for j in range(i + 1, len(top_topics)):\n",
    "            sim = topic_cosine_similarity(top_topics[i], top_topics[j])\n",
    "            if sim > threshold:\n",
    "                # Discard the one with the highest ID\n",
    "                discard_index = i if top_topics[i] > top_topics[j] else j\n",
    "                top_topics.pop(discard_index)\n",
    "                # Pull the next topic from valid_topics, if available\n",
    "                if index < len(valid_topics):\n",
    "                    new_topic = valid_topics.iloc[index].Topic\n",
    "                    top_topics.append(new_topic)\n",
    "                    index += 1\n",
    "                changed = True\n",
    "                break\n",
    "        if changed:\n",
    "            break\n",
    "    if not changed:\n",
    "        break\n",
    "\n",
    "top_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_models = {}\n",
    "divided_reviews = []\n",
    "\n",
    "for topic_id in top_topics:\n",
    "    # Collect reviews for this specific topic\n",
    "    topic_docs = [\n",
    "        reviews[i]\n",
    "        for i, t in enumerate(topic_model.get_document_info(reviews)['Topic'])\n",
    "        if t == topic_id\n",
    "    ]\n",
    "    divided_reviews.append(topic_docs)\n",
    "    print(f\"Topic {topic_id} has {len(topic_docs)} documents\")\n",
    "    # Create a new BERTopic model and fit only these documents\n",
    "    model = BERTopic()\n",
    "    model.fit_transform(topic_docs)\n",
    "    new_models[topic_id] = model\n",
    "\n",
    "new_models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    api_version=\"2023-07-01-preview\",\n",
    "    azure_endpoint=\"https://derai-vision.openai.azure.com/\", \n",
    ")\n",
    "\n",
    "model = \"gpt-4o-mini\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_keyword(cluster_words):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a helpful expert summarizer that identifies and generates a concise, broad topic word for each cluster of words.\\n\"\n",
    "                \"The topic word should capture the essence of all the words in the cluster.\\n\"\n",
    "                \"Merge similar or related words into a single, broader category.\\n\"\n",
    "                \"Use singular words unless a plural form is necessary.\\n\"\n",
    "                \"Use only one word. 2 or 3 words can be used only when they are part of a composite word and are better to represent the idea of the topic (e.g.: ease of use).\\n\"\n",
    "                \"If you identify a verb as a topic, use the noun version (e.g., use 'order' instead of 'ordering').\\n\"\n",
    "                \"Generalize the topic word; for example, if you encounter 'saleswoman' or 'salesman', abstract it to 'staff'.\\n\"\n",
    "                \"Provide the output as a single word.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Please read the following cluster of words carefully and generate a single topic word that captures the essence of all the words.\\n\"\n",
    "                \"The topic word should be broad and general, capturing the essence of the cluster's main points without being overly specific or redundant.\\n\"\n",
    "                \"The topics could be either nouns that refer to a certain characteristic of the product or specific features or parts of the product (e.g.: click & collect, email redeem, etc.)\\n\"\n",
    "                \"The probabilities indicate how important a certain word is in the topic.\\n\"\n",
    "                f\"Cluster: {', '.join([f'{word} ({prob:.4f})' for word, prob in cluster_words])}\\n\"\n",
    "                \"Topic word(s):\"\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = ' '\n",
    "    \n",
    "    # Generate the topic word using the language model\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=5,\n",
    "        temperature=0.4,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "    )\n",
    "\n",
    "    # Extract and return the topic word\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords = {}\n",
    "\n",
    "for topic in top_topics:\n",
    "    topic_name = topic_info[topic_info['Topic'] == topic]['Name'].values[0]\n",
    "    topic_keywords[topic_name] = get_topic_keyword(topic_model.get_topic(topic))\n",
    "\n",
    "print(topic_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "topic_infos = []\n",
    "final_subtopics = []\n",
    "\n",
    "for i, model in enumerate(new_models.values()):\n",
    "    topic_infos.append(model.get_topic_info())\n",
    "    subtopics = model.get_topic_freq()\n",
    "    print(subtopics)\n",
    "    valid_subtopics = subtopics[subtopics.Topic != -1].sort_values(by=\"Count\", ascending=False)\n",
    "    top_subtopics = valid_subtopics.head(4).Topic.tolist()\n",
    "    index = 4  # Next topic candidate\n",
    "\n",
    "    while True:\n",
    "        changed = False\n",
    "        for i in range(len(top_subtopics)):\n",
    "            for j in range(i + 1, len(top_subtopics)):\n",
    "                sim = topic_cosine_similarity(top_subtopics[i], top_subtopics[j])\n",
    "                if sim > threshold:\n",
    "                    # Discard the one with the highest ID\n",
    "                    discard_index = i if top_subtopics[i] > top_subtopics[j] else j\n",
    "                    top_subtopics.pop(discard_index)\n",
    "                    # Pull the next topic from valid_subtopics, if available\n",
    "                    if index < len(valid_subtopics):\n",
    "                        new_topic = valid_subtopics.iloc[index].Topic\n",
    "                        top_subtopics.append(new_topic)\n",
    "                        index += 1\n",
    "                    changed = True\n",
    "                    break\n",
    "            if changed:\n",
    "                break\n",
    "        if not changed:\n",
    "            break\n",
    "\n",
    "    final_subtopics.append(top_subtopics)\n",
    "\n",
    "print(f\"Final subtopics for all models: {final_subtopics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_subtopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "def get_subtopic_keyword(topic_keyword, cluster_words):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a helpful expert summarizer that identifies and generates a concise, broad subtopic word for each cluster of words.\\n\"\n",
    "                \"The topic word should capture the essence of all the words in the cluster.\\n\"\n",
    "                \"The words you choose can be specific, since they are a specialization of a broader topic word.\\n\" \n",
    "                \"Use singular words unless a plural form is necessary.\\n\"                \n",
    "                \"Use only one word unless 2 or 3 words are better to represent the idea of the subtopic.\\n\"\n",
    "                \"If you identify a verb as a subtopic, use the noun version (e.g., use 'order' instead of 'ordering').\\n\"\n",
    "                \"Generalize the topic word; for example, if you encounter 'saleswoman' or 'salesman', abstract it to 'staff'.\\n\"\n",
    "                f\"Provide the output as: '{topic_keyword} - <Subtopic word>'.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Please read the following cluster of words carefully and generate a single subtopic word that captures the essence of all the words.\\n\"\n",
    "                \"The subtopic is a specification of the broader topic, therefore it should be about an aspect that the customers mention and that is related to the broader topic.\\n\"\n",
    "                \"The topics could be either nouns that refers to a certain characteristic of the product of spefic features or parts of the product (e.g.: click & collect, email redeem, etc.)\\n\"\n",
    "                \"The probabilities indicate how important a certain word is in the topic.\\n\"\n",
    "                f\"The broader topic word is: {topic_keyword}\\n\"\n",
    "                f\"Cluster: {', '.join([f'{word} ({prob:.4f})' for word, prob in cluster_words])}\\n\"\n",
    "                \"Topic word(s):\"\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = ' '\n",
    "    model = \"gpt-4o-mini\" \n",
    "    # Generate the topic word using the language model\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=10,\n",
    "        temperature=0.4,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "    )\n",
    "\n",
    "    # Extract and return the topic word\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtopic_keywords = {}\n",
    "\n",
    "for i, subtopics in enumerate(final_subtopics):\n",
    "    # Get the main topic name by matching index i in topic_keywords\n",
    "    parent_topic_name = list(topic_keywords.values())[i]\n",
    "    print(\"parent_topic_name: \", parent_topic_name)\n",
    "    parent_topic_id = top_topics[i]\n",
    "    current_topic_info = topic_infos[i]\n",
    "\n",
    "    # Use the corresponding BERTopic model from new_models for this main topic.\n",
    "    subtopic_model = new_models[parent_topic_id]\n",
    "\n",
    "    subtopic_keywords[list(topic_keywords.items())[i]] = {}\n",
    "\n",
    "    # Generate a keyword for each subtopic\n",
    "    for subtopic_id in subtopics:\n",
    "        subtopic_name = current_topic_info[current_topic_info['Topic'] == subtopic_id]['Name'].values[0]\n",
    "        subtopic_word_probs = subtopic_model.get_topic(subtopic_id)\n",
    "        keyword = get_subtopic_keyword(parent_topic_name, subtopic_word_probs)\n",
    "        subtopic_keywords[list(topic_keywords.items())[i]][subtopic_name] = keyword\n",
    "\n",
    "subtopic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def create_json_structure(subtopics_structure, output_file):\n",
    "    \"\"\"\n",
    "    Create a JSON structure with topics, subtopics, and reviews, and save it to a file.\n",
    "\n",
    "    Parameters:\n",
    "    - subtopics_structure: Dictionary containing topics and their subtopics.\n",
    "    - subtopic_reviews: Dictionary with subtopic IDs as keys and list of reviews as values.\n",
    "    - output_file: Path to the output JSON file.\n",
    "    \"\"\"\n",
    "    json_structure = {}\n",
    "\n",
    "    for main_topic, subtopics in subtopics_structure.items():\n",
    "        print(subtopics)\n",
    "        topic_name = main_topic[0]\n",
    "        topic_keyword = main_topic[1]\n",
    "        json_structure[main_topic[0]] = {\n",
    "            \"keyword\": topic_keyword,\n",
    "            \"subtopics\": {}\n",
    "        }\n",
    "\n",
    "        print(f\"Processing main topic: {topic_name} - {topic_keyword}\")\n",
    "\n",
    "        for i in range(len(subtopics)):\n",
    "            subtopic_name = list(subtopics.keys())[i]\n",
    "            subtopic_keyword = list(subtopics.values())[i]            \n",
    "\n",
    "            json_structure[main_topic[0]][\"subtopics\"][subtopic_name] = {\n",
    "                \"subtopic_keyword\": subtopic_keyword,\n",
    "                # \"reviews\": reviews # Reviews for each subtopic need to be fetched and added here\n",
    "            }\n",
    "\n",
    "            print(f\"  Subtopic ID: {subtopic_name} - {subtopic_keyword}\")\n",
    "            print(f\"    Keyword: {subtopic_keyword}\")\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(json_structure, f, indent=4)\n",
    "\n",
    "    print(f\"JSON structure saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(divided_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'top8.json'\n",
    "create_json_structure(subtopic_keywords, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run app.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-reviews-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
